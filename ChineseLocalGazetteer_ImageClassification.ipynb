{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons\n",
    "!pip install vit_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, save_img\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from vit_keras import vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Image by Numpy Array\n",
    "#Train Image\n",
    "train_image_npy_path = './train_image.npy' #train_image_npy_path\n",
    "train_image = np.load(train_image_npy_path)\n",
    "#Test Image\n",
    "test_image_npy_path = './test_image.npy' #test_image_npy_path\n",
    "test_image = np.load(test_image_npy_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three-Category Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Label\n",
    "train_label_npy_path = './train_label_3.npy' #train_label_npy_path (Three-Category Dataset)\n",
    "train_label = np.load(train_label_npy_path)\n",
    "#Test Label\n",
    "test_label_npy_path = './test_label_3.npy' #test_label_npy_path (Three-Category Dataset)\n",
    "test_label = np.load(test_label_npy_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nine-Class Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Label\n",
    "train_label_npy_path = './train_label_9.npy' #train_label_npy_path (Nine-Class Dataset)\n",
    "train_label = np.load(train_label_npy_path)\n",
    "#Test Label\n",
    "test_label_npy_path = './test_label_9.npy' #test_label_npy_path (Nine-Class Dataset)\n",
    "test_label = np.load(test_label_npy_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 128\n",
    "classes = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U kecam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SwimT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_cv_attention_models import swin_transformer_v2\n",
    "\n",
    "pretrained = os.path.expanduser('./swin_transformer_v2_base_window16_256_imagenet.h5') #swin_transformer_v2 Model Path\n",
    "base_model = swin_transformer_v2.SwinTransformerV2Base_window16(\n",
    "    input_shape = (img_size, img_size, 3),\n",
    "    pretrained=pretrained,\n",
    "    num_classes=classes\n",
    ")\n",
    "\n",
    "Model = tf.keras.Sequential([\n",
    "        base_model\n",
    "    ],\n",
    "    name = 'swim_transformer')\n",
    "\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DaViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_cv_attention_models import davit\n",
    "\n",
    "pretrained = os.path.expanduser('./davit_b_imagenet.h5') #DaViT Model Path\n",
    "base_model = davit.DaViT_B(\n",
    "    input_shape = (img_size, img_size, 3),\n",
    "    pretrained=pretrained,\n",
    "    num_classes=classes\n",
    ")\n",
    "\n",
    "Model = tf.keras.Sequential([\n",
    "        base_model\n",
    "    ],\n",
    "    name = 'DaViT')\n",
    "\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 777\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call Backs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '' #checkpoint_filepath\n",
    "\n",
    "lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', min_delta=0, factor=0.5, patience=2, min_lr=1e-6)\n",
    "stop = EarlyStopping(monitor=\"val_categorical_accuracy\", min_delta=0, patience=5, restore_best_weights=True)\n",
    "ck = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False , monitor='val_categorical_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "model_callbacks = [lr, ck]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "Model_history = Model.fit(train_image, train_label, validation_split=0.1, epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, verbose=1, callbacks=model_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Accuaracy & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acc\n",
    "acc = Model_history.history['categorical_accuracy']\n",
    "val_acc = Model_history.history['val_categorical_accuracy']\n",
    "\n",
    "#Loss\n",
    "loss = Model_history.history['loss']\n",
    "val_loss = Model_history.history['val_loss']\n",
    "\n",
    "#Acc figure\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(acc, label='Train Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy', linestyle='--')\n",
    "plt.title('Accuracy', fontsize = 16)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.ylabel('Acc', fontsize = 14)\n",
    "plt.legend()\n",
    "\n",
    "#Loss figure\n",
    "plt.subplot(122)\n",
    "plt.plot(loss, label='Train loss')\n",
    "plt.plot(val_loss, label='Validation loss', linestyle='--')\n",
    "plt.title('Loss', fontsize = 16)\n",
    "plt.xlabel('Epoch', fontsize = 14)\n",
    "plt.ylabel('Loss', fontsize = 14)\n",
    "plt.legend()\n",
    "sns.set(style='darkgrid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluatoion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = Model.evaluate(test_image, test_label, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 560\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call Backs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = '' #checkpoint_filepath\n",
    "\n",
    "lr = ReduceLROnPlateau(monitor='val_categorical_accuracy', min_delta=0, factor=0.5, patience=2, min_lr=1e-6)\n",
    "stop = EarlyStopping(monitor=\"val_categorical_accuracy\", min_delta=0, patience=5, restore_best_weights=True)\n",
    "ck = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False , monitor='val_categorical_accuracy', mode='max', save_best_only=True)\n",
    "\n",
    "model_callbacks = [lr, ck]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNE_EPOCHS = 10\n",
    "INITIAL_EPOCHS = Model_history.epoch[-1]+1\n",
    "TOTAL_EPOCHS =  INITIAL_EPOCHS + FINETUNE_EPOCHS\n",
    "\n",
    "Model_history_fine = Model.fit(train_image, train_label, validation_split=0.25, epochs=TOTAL_EPOCHS, initial_epoch=INITIAL_EPOCHS, batch_size=BATCH_SIZE, shuffle=True, verbose=1, callbacks=model_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = Model.evaluate(test_image, test_label, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint_filepath = '' #save_checkpoint_filepath\n",
    "Model.save(save_checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_checkpoint_filepath = '' #load_checkpoint_filepath\n",
    "Model = load_model(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.argmax(test_pred, axis=1)\n",
    "test_label_true = np.argmax(test_label, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names_three = ['0(text)', '1(targer map)', '2(non-target graphic)']\n",
    "target_names_nine = ['0(text)', '1(scenic map)', '2(city map)', '3(administrative map)', '4(star map)', '5(photograph)', '6(human figure)', '7(building)', '8(object)']\n",
    "\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(test_label_true, test_pred, target_names=target_names_nine, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(test_label_true, test_pred)\n",
    "display_labels_three=['0(text)', '1(target map)', '2(non-target graphic)']\n",
    "display_labels_nine=['0(text)', '1(scenic)', '2(city)', '3(adm)', '4(star)', '5(photo)', '6(human)', '7(building)', '8(object)']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels_nine)\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "plt.title('ConfusionMatrix', fontsize=20)\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='g', ax=ax)\n",
    "sns.set(style='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveImage(a,b,i):\n",
    "    error_example_dir = '' #error_example_dir\n",
    "    dir = error_example_dir + str(a) + \"_\" + str(b) + \"/\"\n",
    "\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    \n",
    "    test_label_dir = ''\n",
    "    source = test_label_dir + str(10001+i) + \".jpg\"\n",
    "    destination = dir + str(10001+i) + \".jpg\"\n",
    "    shutil.copyfile(source,destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(test_label_true)):\n",
    "    a = test_label_true[i]\n",
    "    b = test_pred[i]\n",
    "    if a != b:\n",
    "        saveImage(a,b,i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
